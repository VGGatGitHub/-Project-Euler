{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "chatbot_v5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VGGatGitHub/Project-Euler/blob/master/chatbot_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIF2hQ5wGsqg",
        "colab_type": "text"
      },
      "source": [
        "https://www.kaggle.com/datasets?search=nq-train\n",
        "\n",
        "V2: added comads to look at the structure of the train.json file and to assess the %s.\n",
        "\n",
        "V3: changing the code to do tarining for long_answesrs or short_answers using training_for_long_answer switch.\n",
        "\n",
        "V4: reading in json file produced from jsonl using jsonl2json.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xPDM8ocY74i",
        "colab_type": "code",
        "outputId": "97e4983b-d0be-4358-a513-d822bc58ffcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ed-1NnYIGsqj",
        "colab_type": "code",
        "outputId": "a8a06a07-85d1-4a9b-d071-98815de7d4c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sys \n",
        "import os\n",
        "print(os.getcwd())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QByI39lkVt8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#VGG define the foldre to inspect for files \n",
        "path='/content/drive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3ej-f3iYGsqm",
        "colab_type": "code",
        "outputId": "697ce174-492d-42cc-80b3-bf8fd152e662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#make sure the file train.json is in the correct directory \n",
        "\n",
        "for dirname, _, filenames in os.walk(path):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/chatbot_v2 (1).ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/train.json\n",
            "/content/drive/My Drive/Colab Notebooks/loding_files.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/Copy of qa.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/chatbot_v2.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/simplified-nq-train.jsonl\n",
            "/content/drive/My Drive/Colab Notebooks/chatbot_v3.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/train200.json\n",
            "/content/drive/My Drive/Colab Notebooks/text_utils.py\n",
            "/content/drive/My Drive/Colab Notebooks/jsonl2json.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/chatbot_v5.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_S1byB1WPph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#VGG read in training data\n",
        "file_to_read=path+'train200.json' \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS2xGhZCoFxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VGG\n",
        "# you may need to get the file text_utils.py from \n",
        "# https://github.com/VGGatGitHub/natural-questions\n",
        "#\n",
        "\n",
        "sys.path.append(os.path.abspath(path))\n",
        "from text_utils import *\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EhFsBX5aKR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#VGG The cell has been removed since now teh data is analized in the jsonl2json.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pgXArJmIaiv",
        "colab_type": "code",
        "outputId": "80c29fb4-5089-482e-dd3f-d9f09e4290ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7bPY5ic2eC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbW-oSFI2mFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#VGG make sure the file_to_read has been defined above! \n",
        "\n",
        "UNKNOWN = \"<UNKNOWN>\"\n",
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset():\n",
        "    source = []\n",
        "    target = []\n",
        "    context = []\n",
        "\n",
        "    n_short_answers=0 #VGG\n",
        "    n_long=0\n",
        "    training_for_long_answer = False #True #False \n",
        "    \n",
        "    with open(file_to_read) as json_file: #VGG\n",
        "        data = json.load(json_file)\n",
        "\n",
        "        for nq_doc in data:\n",
        "            doc = simplify_nq_example(nq_doc) #VGG for jsonl formated file\n",
        "\n",
        "            question_text = doc['question_text']\n",
        "            document_text = doc['document_text'].split()\n",
        "            long_answer_candidates = doc['long_answer_candidates']\n",
        "            annotations = doc['annotations'][0]\n",
        "            \n",
        "            if annotations['long_answer']['start_token'] < annotations['long_answer']['end_token']:\n",
        "                \n",
        "                n_long+=1\n",
        "                long_answer = \" \".join(document_text[annotations['long_answer']['start_token']:\n",
        "                                                     annotations['long_answer']['end_token']])\n",
        "                                      \n",
        "                if len(annotations['short_answers']) > 0:\n",
        "                    start_token = annotations['short_answers'][0]['start_token']\n",
        "                    end_token = annotations['short_answers'][0]['end_token']\n",
        "                    short_answer = \" \".join(document_text[start_token:end_token])\n",
        "                    n_short_answers+=1 #VGG\n",
        "                else:\n",
        "                    short_answer = UNKNOWN\n",
        "                \n",
        "                #VGG V3\n",
        "                if training_for_long_answer :\n",
        "                    short_answer=long_answer #VGG V3 change - make the target to be the long answer instead of the short answer \n",
        "                    for posibilities in long_answer_candidates:\n",
        "                        if posibilities[\"top_level\"]:\n",
        "                            start_token = posibilities['start_token']\n",
        "                            end_token = posibilities['end_token']                    \n",
        "                            posibility = \" \".join(document_text[start_token:end_token])\n",
        "                            context.append(preprocess_sentence(posibility))\n",
        "                else:\n",
        "                    context.append(preprocess_sentence(long_answer))\n",
        "                \n",
        "                context = [] #VGG it seems to work better!\n",
        "\n",
        "                source.append(preprocess_sentence(question_text))\n",
        "                target.append(preprocess_sentence(short_answer))\n",
        "#VGG                \n",
        "        print(n_short_answers,\"short answers out of\",n_long, \"possible long answers, rate is\",100*n_short_answers/n_long,\"%\")    \n",
        "    return target, source, context\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4ecf3676-acd1-45e5-9459-3f9030b8b059",
        "_cell_guid": "3f83746e-4fa4-4e24-98cb-2f0df139a6af",
        "trusted": true,
        "id": "jevAjFFiGsqr",
        "colab_type": "code",
        "outputId": "a630bd07-c372-45f8-9181-a0117eacb176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "    \n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "def load_dataset():\n",
        "    # creating cleaned input, output pairs\n",
        "    targ_lang, inp_lang, context_lang = create_dataset()\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "    \n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "    \n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "    \n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "# Try experimenting with the size of that dataset\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset()\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)    \n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 16 #VGG\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)    \n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "    \n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ = tf.multiply(loss_, mask)\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "\n",
        "#VGG uncommented for possible checkpoint saving later \n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)  \n",
        "\n",
        "                                 \n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss\n",
        "  \n",
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index.get(i, 0) for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, attention_plot\n",
        "    \n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70 short answers out of 101 possible long answers, rate is 69.3069306930693 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OMsxPEhOGsqt",
        "colab_type": "code",
        "outputId": "979723f8-ae22-4463-eb9b-423d0d9db248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "#VGG for epoch in range(EPOCHS):\n",
        "epoch=-1\n",
        "batch_loss=1\n",
        "while (epoch < EPOCHS) and (batch_loss > 0.0001):\n",
        "  epoch+=1\n",
        "\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  '''\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "  '''   \n",
        "\n",
        "  print('Epoch {} Estimated Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "  print('Time taken for this epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.7373\n",
            "Epoch 1 Batch 1 Loss 0.6869\n",
            "Epoch 1 Batch 2 Loss 1.1755\n",
            "Epoch 1 Batch 3 Loss 0.5746\n",
            "Epoch 1 Batch 4 Loss 0.3838\n",
            "Epoch 1 Estimated Loss 0.7116\n",
            "Time taken for this epoch 32.27378964424133 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.6402\n",
            "Epoch 2 Batch 1 Loss 0.3796\n",
            "Epoch 2 Batch 2 Loss 0.4651\n",
            "Epoch 2 Batch 3 Loss 0.8707\n",
            "Epoch 2 Batch 4 Loss 0.7203\n",
            "Epoch 2 Estimated Loss 0.6152\n",
            "Time taken for this epoch 2.814340353012085 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.4950\n",
            "Epoch 3 Batch 1 Loss 0.7959\n",
            "Epoch 3 Batch 2 Loss 0.5549\n",
            "Epoch 3 Batch 3 Loss 0.6171\n",
            "Epoch 3 Batch 4 Loss 0.3881\n",
            "Epoch 3 Estimated Loss 0.5702\n",
            "Time taken for this epoch 2.844754934310913 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.5510\n",
            "Epoch 4 Batch 1 Loss 0.4874\n",
            "Epoch 4 Batch 2 Loss 0.5229\n",
            "Epoch 4 Batch 3 Loss 0.6347\n",
            "Epoch 4 Batch 4 Loss 0.5293\n",
            "Epoch 4 Estimated Loss 0.5451\n",
            "Time taken for this epoch 2.801762342453003 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.5385\n",
            "Epoch 5 Batch 1 Loss 0.4333\n",
            "Epoch 5 Batch 2 Loss 0.4270\n",
            "Epoch 5 Batch 3 Loss 0.7786\n",
            "Epoch 5 Batch 4 Loss 0.4313\n",
            "Epoch 5 Estimated Loss 0.5217\n",
            "Time taken for this epoch 2.829185962677002 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.4813\n",
            "Epoch 6 Batch 1 Loss 0.6105\n",
            "Epoch 6 Batch 2 Loss 0.3123\n",
            "Epoch 6 Batch 3 Loss 0.4602\n",
            "Epoch 6 Batch 4 Loss 0.6730\n",
            "Epoch 6 Estimated Loss 0.5075\n",
            "Time taken for this epoch 2.857942819595337 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.3059\n",
            "Epoch 7 Batch 1 Loss 0.5458\n",
            "Epoch 7 Batch 2 Loss 0.5598\n",
            "Epoch 7 Batch 3 Loss 0.4281\n",
            "Epoch 7 Batch 4 Loss 0.6089\n",
            "Epoch 7 Estimated Loss 0.4897\n",
            "Time taken for this epoch 2.860832452774048 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.2471\n",
            "Epoch 8 Batch 1 Loss 0.5987\n",
            "Epoch 8 Batch 2 Loss 0.5175\n",
            "Epoch 8 Batch 3 Loss 0.6097\n",
            "Epoch 8 Batch 4 Loss 0.3929\n",
            "Epoch 8 Estimated Loss 0.4732\n",
            "Time taken for this epoch 2.853792667388916 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.6189\n",
            "Epoch 9 Batch 1 Loss 0.4326\n",
            "Epoch 9 Batch 2 Loss 0.3834\n",
            "Epoch 9 Batch 3 Loss 0.4550\n",
            "Epoch 9 Batch 4 Loss 0.3978\n",
            "Epoch 9 Estimated Loss 0.4575\n",
            "Time taken for this epoch 2.846949815750122 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.5286\n",
            "Epoch 10 Batch 1 Loss 0.4983\n",
            "Epoch 10 Batch 2 Loss 0.4294\n",
            "Epoch 10 Batch 3 Loss 0.3746\n",
            "Epoch 10 Batch 4 Loss 0.3828\n",
            "Epoch 10 Estimated Loss 0.4427\n",
            "Time taken for this epoch 2.841872215270996 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.3822\n",
            "Epoch 11 Batch 1 Loss 0.4991\n",
            "Epoch 11 Batch 2 Loss 0.3539\n",
            "Epoch 11 Batch 3 Loss 0.3428\n",
            "Epoch 11 Batch 4 Loss 0.5497\n",
            "Epoch 11 Estimated Loss 0.4255\n",
            "Time taken for this epoch 2.829620838165283 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.4830\n",
            "Epoch 12 Batch 1 Loss 0.2267\n",
            "Epoch 12 Batch 2 Loss 0.6544\n",
            "Epoch 12 Batch 3 Loss 0.2875\n",
            "Epoch 12 Batch 4 Loss 0.4074\n",
            "Epoch 12 Estimated Loss 0.4118\n",
            "Time taken for this epoch 2.8830907344818115 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.5994\n",
            "Epoch 13 Batch 1 Loss 0.2700\n",
            "Epoch 13 Batch 2 Loss 0.6744\n",
            "Epoch 13 Batch 3 Loss 0.2612\n",
            "Epoch 13 Batch 4 Loss 0.2019\n",
            "Epoch 13 Estimated Loss 0.4014\n",
            "Time taken for this epoch 2.8345508575439453 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.2857\n",
            "Epoch 14 Batch 1 Loss 0.6472\n",
            "Epoch 14 Batch 2 Loss 0.3036\n",
            "Epoch 14 Batch 3 Loss 0.4478\n",
            "Epoch 14 Batch 4 Loss 0.2080\n",
            "Epoch 14 Estimated Loss 0.3785\n",
            "Time taken for this epoch 2.8151347637176514 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.3548\n",
            "Epoch 15 Batch 1 Loss 0.5130\n",
            "Epoch 15 Batch 2 Loss 0.5063\n",
            "Epoch 15 Batch 3 Loss 0.1271\n",
            "Epoch 15 Batch 4 Loss 0.2981\n",
            "Epoch 15 Estimated Loss 0.3599\n",
            "Time taken for this epoch 2.785513162612915 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.3126\n",
            "Epoch 16 Batch 1 Loss 0.3827\n",
            "Epoch 16 Batch 2 Loss 0.4292\n",
            "Epoch 16 Batch 3 Loss 0.2245\n",
            "Epoch 16 Batch 4 Loss 0.3143\n",
            "Epoch 16 Estimated Loss 0.3327\n",
            "Time taken for this epoch 2.889253616333008 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.3230\n",
            "Epoch 17 Batch 1 Loss 0.3854\n",
            "Epoch 17 Batch 2 Loss 0.2715\n",
            "Epoch 17 Batch 3 Loss 0.2777\n",
            "Epoch 17 Batch 4 Loss 0.2744\n",
            "Epoch 17 Estimated Loss 0.3064\n",
            "Time taken for this epoch 2.777543783187866 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.3272\n",
            "Epoch 18 Batch 1 Loss 0.4427\n",
            "Epoch 18 Batch 2 Loss 0.2285\n",
            "Epoch 18 Batch 3 Loss 0.1644\n",
            "Epoch 18 Batch 4 Loss 0.2243\n",
            "Epoch 18 Estimated Loss 0.2774\n",
            "Time taken for this epoch 2.819586753845215 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.2018\n",
            "Epoch 19 Batch 1 Loss 0.3139\n",
            "Epoch 19 Batch 2 Loss 0.1809\n",
            "Epoch 19 Batch 3 Loss 0.1948\n",
            "Epoch 19 Batch 4 Loss 0.3624\n",
            "Epoch 19 Estimated Loss 0.2508\n",
            "Time taken for this epoch 2.8621022701263428 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.2705\n",
            "Epoch 20 Batch 1 Loss 0.1527\n",
            "Epoch 20 Batch 2 Loss 0.4141\n",
            "Epoch 20 Batch 3 Loss 0.1247\n",
            "Epoch 20 Batch 4 Loss 0.2010\n",
            "Epoch 20 Estimated Loss 0.2326\n",
            "Time taken for this epoch 2.787546396255493 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.1197\n",
            "Epoch 21 Batch 1 Loss 0.1364\n",
            "Epoch 21 Batch 2 Loss 0.4840\n",
            "Epoch 21 Batch 3 Loss 0.2400\n",
            "Epoch 21 Batch 4 Loss 0.2670\n",
            "Epoch 21 Estimated Loss 0.2494\n",
            "Time taken for this epoch 2.7442026138305664 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.2790\n",
            "Epoch 22 Batch 1 Loss 0.2260\n",
            "Epoch 22 Batch 2 Loss 0.2844\n",
            "Epoch 22 Batch 3 Loss 0.1658\n",
            "Epoch 22 Batch 4 Loss 0.1958\n",
            "Epoch 22 Estimated Loss 0.2302\n",
            "Time taken for this epoch 2.8331942558288574 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.2029\n",
            "Epoch 23 Batch 1 Loss 0.1941\n",
            "Epoch 23 Batch 2 Loss 0.2223\n",
            "Epoch 23 Batch 3 Loss 0.2159\n",
            "Epoch 23 Batch 4 Loss 0.1834\n",
            "Epoch 23 Estimated Loss 0.2037\n",
            "Time taken for this epoch 2.806575298309326 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.1801\n",
            "Epoch 24 Batch 1 Loss 0.1820\n",
            "Epoch 24 Batch 2 Loss 0.1599\n",
            "Epoch 24 Batch 3 Loss 0.2641\n",
            "Epoch 24 Batch 4 Loss 0.0901\n",
            "Epoch 24 Estimated Loss 0.1752\n",
            "Time taken for this epoch 2.8326237201690674 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0926\n",
            "Epoch 25 Batch 1 Loss 0.1436\n",
            "Epoch 25 Batch 2 Loss 0.2238\n",
            "Epoch 25 Batch 3 Loss 0.0980\n",
            "Epoch 25 Batch 4 Loss 0.1945\n",
            "Epoch 25 Estimated Loss 0.1505\n",
            "Time taken for this epoch 2.8251869678497314 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.1401\n",
            "Epoch 26 Batch 1 Loss 0.0778\n",
            "Epoch 26 Batch 2 Loss 0.1113\n",
            "Epoch 26 Batch 3 Loss 0.1346\n",
            "Epoch 26 Batch 4 Loss 0.1906\n",
            "Epoch 26 Estimated Loss 0.1309\n",
            "Time taken for this epoch 2.8734934329986572 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0797\n",
            "Epoch 27 Batch 1 Loss 0.1209\n",
            "Epoch 27 Batch 2 Loss 0.1315\n",
            "Epoch 27 Batch 3 Loss 0.1269\n",
            "Epoch 27 Batch 4 Loss 0.1285\n",
            "Epoch 27 Estimated Loss 0.1175\n",
            "Time taken for this epoch 2.7855148315429688 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.1488\n",
            "Epoch 28 Batch 1 Loss 0.0572\n",
            "Epoch 28 Batch 2 Loss 0.1009\n",
            "Epoch 28 Batch 3 Loss 0.1172\n",
            "Epoch 28 Batch 4 Loss 0.0976\n",
            "Epoch 28 Estimated Loss 0.1043\n",
            "Time taken for this epoch 2.9242825508117676 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0608\n",
            "Epoch 29 Batch 1 Loss 0.0546\n",
            "Epoch 29 Batch 2 Loss 0.1234\n",
            "Epoch 29 Batch 3 Loss 0.1466\n",
            "Epoch 29 Batch 4 Loss 0.0720\n",
            "Epoch 29 Estimated Loss 0.0915\n",
            "Time taken for this epoch 2.9277122020721436 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0807\n",
            "Epoch 30 Batch 1 Loss 0.0672\n",
            "Epoch 30 Batch 2 Loss 0.0738\n",
            "Epoch 30 Batch 3 Loss 0.0855\n",
            "Epoch 30 Batch 4 Loss 0.1232\n",
            "Epoch 30 Estimated Loss 0.0861\n",
            "Time taken for this epoch 2.844542980194092 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0543\n",
            "Epoch 31 Batch 1 Loss 0.0647\n",
            "Epoch 31 Batch 2 Loss 0.1322\n",
            "Epoch 31 Batch 3 Loss 0.0735\n",
            "Epoch 31 Batch 4 Loss 0.0854\n",
            "Epoch 31 Estimated Loss 0.0820\n",
            "Time taken for this epoch 2.8241002559661865 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0477\n",
            "Epoch 32 Batch 1 Loss 0.0842\n",
            "Epoch 32 Batch 2 Loss 0.1319\n",
            "Epoch 32 Batch 3 Loss 0.0789\n",
            "Epoch 32 Batch 4 Loss 0.0719\n",
            "Epoch 32 Estimated Loss 0.0830\n",
            "Time taken for this epoch 2.852800130844116 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0633\n",
            "Epoch 33 Batch 1 Loss 0.0828\n",
            "Epoch 33 Batch 2 Loss 0.0797\n",
            "Epoch 33 Batch 3 Loss 0.0726\n",
            "Epoch 33 Batch 4 Loss 0.1832\n",
            "Epoch 33 Estimated Loss 0.0963\n",
            "Time taken for this epoch 3.178429126739502 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0561\n",
            "Epoch 34 Batch 1 Loss 0.1161\n",
            "Epoch 34 Batch 2 Loss 0.0821\n",
            "Epoch 34 Batch 3 Loss 0.0726\n",
            "Epoch 34 Batch 4 Loss 0.0868\n",
            "Epoch 34 Estimated Loss 0.0827\n",
            "Time taken for this epoch 3.155519723892212 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0526\n",
            "Epoch 35 Batch 1 Loss 0.0549\n",
            "Epoch 35 Batch 2 Loss 0.0753\n",
            "Epoch 35 Batch 3 Loss 0.0780\n",
            "Epoch 35 Batch 4 Loss 0.1266\n",
            "Epoch 35 Estimated Loss 0.0775\n",
            "Time taken for this epoch 3.1671946048736572 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0471\n",
            "Epoch 36 Batch 1 Loss 0.0621\n",
            "Epoch 36 Batch 2 Loss 0.0775\n",
            "Epoch 36 Batch 3 Loss 0.0761\n",
            "Epoch 36 Batch 4 Loss 0.0688\n",
            "Epoch 36 Estimated Loss 0.0663\n",
            "Time taken for this epoch 2.902738332748413 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0383\n",
            "Epoch 37 Batch 1 Loss 0.0433\n",
            "Epoch 37 Batch 2 Loss 0.0622\n",
            "Epoch 37 Batch 3 Loss 0.0569\n",
            "Epoch 37 Batch 4 Loss 0.1112\n",
            "Epoch 37 Estimated Loss 0.0624\n",
            "Time taken for this epoch 2.8212170600891113 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0500\n",
            "Epoch 38 Batch 1 Loss 0.0401\n",
            "Epoch 38 Batch 2 Loss 0.0350\n",
            "Epoch 38 Batch 3 Loss 0.0672\n",
            "Epoch 38 Batch 4 Loss 0.0862\n",
            "Epoch 38 Estimated Loss 0.0557\n",
            "Time taken for this epoch 2.8734047412872314 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0496\n",
            "Epoch 39 Batch 1 Loss 0.0504\n",
            "Epoch 39 Batch 2 Loss 0.0501\n",
            "Epoch 39 Batch 3 Loss 0.0507\n",
            "Epoch 39 Batch 4 Loss 0.0483\n",
            "Epoch 39 Estimated Loss 0.0498\n",
            "Time taken for this epoch 2.8362677097320557 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0406\n",
            "Epoch 40 Batch 1 Loss 0.0388\n",
            "Epoch 40 Batch 2 Loss 0.0563\n",
            "Epoch 40 Batch 3 Loss 0.0393\n",
            "Epoch 40 Batch 4 Loss 0.0560\n",
            "Epoch 40 Estimated Loss 0.0462\n",
            "Time taken for this epoch 2.9066927433013916 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0593\n",
            "Epoch 41 Batch 1 Loss 0.0395\n",
            "Epoch 41 Batch 2 Loss 0.0371\n",
            "Epoch 41 Batch 3 Loss 0.0392\n",
            "Epoch 41 Batch 4 Loss 0.0290\n",
            "Epoch 41 Estimated Loss 0.0408\n",
            "Time taken for this epoch 2.9166247844696045 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0248\n",
            "Epoch 42 Batch 1 Loss 0.0294\n",
            "Epoch 42 Batch 2 Loss 0.0298\n",
            "Epoch 42 Batch 3 Loss 0.0673\n",
            "Epoch 42 Batch 4 Loss 0.0380\n",
            "Epoch 42 Estimated Loss 0.0379\n",
            "Time taken for this epoch 2.9052107334136963 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0355\n",
            "Epoch 43 Batch 1 Loss 0.0280\n",
            "Epoch 43 Batch 2 Loss 0.0205\n",
            "Epoch 43 Batch 3 Loss 0.0392\n",
            "Epoch 43 Batch 4 Loss 0.0636\n",
            "Epoch 43 Estimated Loss 0.0374\n",
            "Time taken for this epoch 2.837881326675415 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0248\n",
            "Epoch 44 Batch 1 Loss 0.0483\n",
            "Epoch 44 Batch 2 Loss 0.0501\n",
            "Epoch 44 Batch 3 Loss 0.0225\n",
            "Epoch 44 Batch 4 Loss 0.0390\n",
            "Epoch 44 Estimated Loss 0.0370\n",
            "Time taken for this epoch 2.8346869945526123 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0544\n",
            "Epoch 45 Batch 1 Loss 0.0261\n",
            "Epoch 45 Batch 2 Loss 0.0173\n",
            "Epoch 45 Batch 3 Loss 0.0375\n",
            "Epoch 45 Batch 4 Loss 0.0576\n",
            "Epoch 45 Estimated Loss 0.0386\n",
            "Time taken for this epoch 2.8058671951293945 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0355\n",
            "Epoch 46 Batch 1 Loss 0.0105\n",
            "Epoch 46 Batch 2 Loss 0.0314\n",
            "Epoch 46 Batch 3 Loss 0.0685\n",
            "Epoch 46 Batch 4 Loss 0.0254\n",
            "Epoch 46 Estimated Loss 0.0343\n",
            "Time taken for this epoch 2.789968252182007 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0234\n",
            "Epoch 47 Batch 1 Loss 0.0218\n",
            "Epoch 47 Batch 2 Loss 0.0604\n",
            "Epoch 47 Batch 3 Loss 0.0336\n",
            "Epoch 47 Batch 4 Loss 0.0241\n",
            "Epoch 47 Estimated Loss 0.0327\n",
            "Time taken for this epoch 2.8815391063690186 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0231\n",
            "Epoch 48 Batch 1 Loss 0.0165\n",
            "Epoch 48 Batch 2 Loss 0.0181\n",
            "Epoch 48 Batch 3 Loss 0.0135\n",
            "Epoch 48 Batch 4 Loss 0.0712\n",
            "Epoch 48 Estimated Loss 0.0285\n",
            "Time taken for this epoch 2.9035286903381348 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0207\n",
            "Epoch 49 Batch 1 Loss 0.0190\n",
            "Epoch 49 Batch 2 Loss 0.0191\n",
            "Epoch 49 Batch 3 Loss 0.0220\n",
            "Epoch 49 Batch 4 Loss 0.0506\n",
            "Epoch 49 Estimated Loss 0.0262\n",
            "Time taken for this epoch 2.851620674133301 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0177\n",
            "Epoch 50 Batch 1 Loss 0.0090\n",
            "Epoch 50 Batch 2 Loss 0.0353\n",
            "Epoch 50 Batch 3 Loss 0.0353\n",
            "Epoch 50 Batch 4 Loss 0.0162\n",
            "Epoch 50 Estimated Loss 0.0227\n",
            "Time taken for this epoch 2.867305278778076 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0183\n",
            "Epoch 51 Batch 1 Loss 0.0045\n",
            "Epoch 51 Batch 2 Loss 0.0256\n",
            "Epoch 51 Batch 3 Loss 0.0315\n",
            "Epoch 51 Batch 4 Loss 0.0133\n",
            "Epoch 51 Estimated Loss 0.0186\n",
            "Time taken for this epoch 2.814314126968384 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.0127\n",
            "Epoch 52 Batch 1 Loss 0.0169\n",
            "Epoch 52 Batch 2 Loss 0.0100\n",
            "Epoch 52 Batch 3 Loss 0.0269\n",
            "Epoch 52 Batch 4 Loss 0.0080\n",
            "Epoch 52 Estimated Loss 0.0149\n",
            "Time taken for this epoch 2.8699519634246826 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.0173\n",
            "Epoch 53 Batch 1 Loss 0.0100\n",
            "Epoch 53 Batch 2 Loss 0.0120\n",
            "Epoch 53 Batch 3 Loss 0.0123\n",
            "Epoch 53 Batch 4 Loss 0.0194\n",
            "Epoch 53 Estimated Loss 0.0142\n",
            "Time taken for this epoch 2.8419065475463867 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.0130\n",
            "Epoch 54 Batch 1 Loss 0.0273\n",
            "Epoch 54 Batch 2 Loss 0.0043\n",
            "Epoch 54 Batch 3 Loss 0.0103\n",
            "Epoch 54 Batch 4 Loss 0.0159\n",
            "Epoch 54 Estimated Loss 0.0142\n",
            "Time taken for this epoch 2.829906940460205 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0045\n",
            "Epoch 55 Batch 1 Loss 0.0115\n",
            "Epoch 55 Batch 2 Loss 0.0293\n",
            "Epoch 55 Batch 3 Loss 0.0051\n",
            "Epoch 55 Batch 4 Loss 0.0280\n",
            "Epoch 55 Estimated Loss 0.0157\n",
            "Time taken for this epoch 2.914435625076294 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.0148\n",
            "Epoch 56 Batch 1 Loss 0.0070\n",
            "Epoch 56 Batch 2 Loss 0.0022\n",
            "Epoch 56 Batch 3 Loss 0.0255\n",
            "Epoch 56 Batch 4 Loss 0.0156\n",
            "Epoch 56 Estimated Loss 0.0130\n",
            "Time taken for this epoch 2.785184144973755 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0113\n",
            "Epoch 57 Batch 1 Loss 0.0266\n",
            "Epoch 57 Batch 2 Loss 0.0169\n",
            "Epoch 57 Batch 3 Loss 0.0102\n",
            "Epoch 57 Batch 4 Loss 0.0101\n",
            "Epoch 57 Estimated Loss 0.0150\n",
            "Time taken for this epoch 2.844271659851074 sec\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.0066\n",
            "Epoch 58 Batch 1 Loss 0.0119\n",
            "Epoch 58 Batch 2 Loss 0.0089\n",
            "Epoch 58 Batch 3 Loss 0.0189\n",
            "Epoch 58 Batch 4 Loss 0.0152\n",
            "Epoch 58 Estimated Loss 0.0123\n",
            "Time taken for this epoch 2.897331476211548 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.0033\n",
            "Epoch 59 Batch 1 Loss 0.0178\n",
            "Epoch 59 Batch 2 Loss 0.0106\n",
            "Epoch 59 Batch 3 Loss 0.0073\n",
            "Epoch 59 Batch 4 Loss 0.0243\n",
            "Epoch 59 Estimated Loss 0.0127\n",
            "Time taken for this epoch 2.808640956878662 sec\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.0066\n",
            "Epoch 60 Batch 1 Loss 0.0315\n",
            "Epoch 60 Batch 2 Loss 0.0153\n",
            "Epoch 60 Batch 3 Loss 0.0020\n",
            "Epoch 60 Batch 4 Loss 0.0172\n",
            "Epoch 60 Estimated Loss 0.0145\n",
            "Time taken for this epoch 2.7811005115509033 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.0125\n",
            "Epoch 61 Batch 1 Loss 0.0112\n",
            "Epoch 61 Batch 2 Loss 0.0265\n",
            "Epoch 61 Batch 3 Loss 0.0153\n",
            "Epoch 61 Batch 4 Loss 0.0040\n",
            "Epoch 61 Estimated Loss 0.0139\n",
            "Time taken for this epoch 2.983563184738159 sec\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.0148\n",
            "Epoch 62 Batch 1 Loss 0.0021\n",
            "Epoch 62 Batch 2 Loss 0.0024\n",
            "Epoch 62 Batch 3 Loss 0.0306\n",
            "Epoch 62 Batch 4 Loss 0.0405\n",
            "Epoch 62 Estimated Loss 0.0181\n",
            "Time taken for this epoch 2.9208576679229736 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.0259\n",
            "Epoch 63 Batch 1 Loss 0.0169\n",
            "Epoch 63 Batch 2 Loss 0.0044\n",
            "Epoch 63 Batch 3 Loss 0.0166\n",
            "Epoch 63 Batch 4 Loss 0.0111\n",
            "Epoch 63 Estimated Loss 0.0150\n",
            "Time taken for this epoch 2.8397104740142822 sec\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.0143\n",
            "Epoch 64 Batch 1 Loss 0.0034\n",
            "Epoch 64 Batch 2 Loss 0.0106\n",
            "Epoch 64 Batch 3 Loss 0.0508\n",
            "Epoch 64 Batch 4 Loss 0.0118\n",
            "Epoch 64 Estimated Loss 0.0182\n",
            "Time taken for this epoch 2.880370616912842 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.0202\n",
            "Epoch 65 Batch 1 Loss 0.0237\n",
            "Epoch 65 Batch 2 Loss 0.0234\n",
            "Epoch 65 Batch 3 Loss 0.0552\n",
            "Epoch 65 Batch 4 Loss 0.0113\n",
            "Epoch 65 Estimated Loss 0.0268\n",
            "Time taken for this epoch 2.8678739070892334 sec\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.0245\n",
            "Epoch 66 Batch 1 Loss 0.0255\n",
            "Epoch 66 Batch 2 Loss 0.0063\n",
            "Epoch 66 Batch 3 Loss 0.0479\n",
            "Epoch 66 Batch 4 Loss 0.0096\n",
            "Epoch 66 Estimated Loss 0.0227\n",
            "Time taken for this epoch 2.787038564682007 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.0222\n",
            "Epoch 67 Batch 1 Loss 0.0111\n",
            "Epoch 67 Batch 2 Loss 0.0422\n",
            "Epoch 67 Batch 3 Loss 0.0188\n",
            "Epoch 67 Batch 4 Loss 0.0232\n",
            "Epoch 67 Estimated Loss 0.0235\n",
            "Time taken for this epoch 2.8913707733154297 sec\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.0084\n",
            "Epoch 68 Batch 1 Loss 0.0338\n",
            "Epoch 68 Batch 2 Loss 0.0211\n",
            "Epoch 68 Batch 3 Loss 0.0401\n",
            "Epoch 68 Batch 4 Loss 0.0146\n",
            "Epoch 68 Estimated Loss 0.0236\n",
            "Time taken for this epoch 2.8198208808898926 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.0150\n",
            "Epoch 69 Batch 1 Loss 0.0101\n",
            "Epoch 69 Batch 2 Loss 0.0331\n",
            "Epoch 69 Batch 3 Loss 0.0212\n",
            "Epoch 69 Batch 4 Loss 0.0241\n",
            "Epoch 69 Estimated Loss 0.0207\n",
            "Time taken for this epoch 2.7945916652679443 sec\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.0091\n",
            "Epoch 70 Batch 1 Loss 0.0067\n",
            "Epoch 70 Batch 2 Loss 0.0128\n",
            "Epoch 70 Batch 3 Loss 0.0216\n",
            "Epoch 70 Batch 4 Loss 0.0330\n",
            "Epoch 70 Estimated Loss 0.0166\n",
            "Time taken for this epoch 2.8162243366241455 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.0029\n",
            "Epoch 71 Batch 1 Loss 0.0028\n",
            "Epoch 71 Batch 2 Loss 0.0137\n",
            "Epoch 71 Batch 3 Loss 0.0281\n",
            "Epoch 71 Batch 4 Loss 0.0273\n",
            "Epoch 71 Estimated Loss 0.0150\n",
            "Time taken for this epoch 2.9743731021881104 sec\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.0101\n",
            "Epoch 72 Batch 1 Loss 0.0097\n",
            "Epoch 72 Batch 2 Loss 0.0172\n",
            "Epoch 72 Batch 3 Loss 0.0100\n",
            "Epoch 72 Batch 4 Loss 0.0074\n",
            "Epoch 72 Estimated Loss 0.0109\n",
            "Time taken for this epoch 2.8210244178771973 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0185\n",
            "Epoch 73 Batch 1 Loss 0.0176\n",
            "Epoch 73 Batch 2 Loss 0.0030\n",
            "Epoch 73 Batch 3 Loss 0.0016\n",
            "Epoch 73 Batch 4 Loss 0.0171\n",
            "Epoch 73 Estimated Loss 0.0115\n",
            "Time taken for this epoch 3.0334324836730957 sec\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.0021\n",
            "Epoch 74 Batch 1 Loss 0.0030\n",
            "Epoch 74 Batch 2 Loss 0.0061\n",
            "Epoch 74 Batch 3 Loss 0.0075\n",
            "Epoch 74 Batch 4 Loss 0.0341\n",
            "Epoch 74 Estimated Loss 0.0106\n",
            "Time taken for this epoch 2.913379430770874 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.0147\n",
            "Epoch 75 Batch 1 Loss 0.0016\n",
            "Epoch 75 Batch 2 Loss 0.0077\n",
            "Epoch 75 Batch 3 Loss 0.0100\n",
            "Epoch 75 Batch 4 Loss 0.0022\n",
            "Epoch 75 Estimated Loss 0.0072\n",
            "Time taken for this epoch 2.9270150661468506 sec\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.0015\n",
            "Epoch 76 Batch 1 Loss 0.0039\n",
            "Epoch 76 Batch 2 Loss 0.0052\n",
            "Epoch 76 Batch 3 Loss 0.0221\n",
            "Epoch 76 Batch 4 Loss 0.0155\n",
            "Epoch 76 Estimated Loss 0.0096\n",
            "Time taken for this epoch 2.8648784160614014 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0088\n",
            "Epoch 77 Batch 1 Loss 0.0046\n",
            "Epoch 77 Batch 2 Loss 0.0012\n",
            "Epoch 77 Batch 3 Loss 0.0013\n",
            "Epoch 77 Batch 4 Loss 0.0117\n",
            "Epoch 77 Estimated Loss 0.0055\n",
            "Time taken for this epoch 2.8985064029693604 sec\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.0045\n",
            "Epoch 78 Batch 1 Loss 0.0022\n",
            "Epoch 78 Batch 2 Loss 0.0040\n",
            "Epoch 78 Batch 3 Loss 0.0131\n",
            "Epoch 78 Batch 4 Loss 0.0006\n",
            "Epoch 78 Estimated Loss 0.0049\n",
            "Time taken for this epoch 3.020059108734131 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0043\n",
            "Epoch 79 Batch 1 Loss 0.0007\n",
            "Epoch 79 Batch 2 Loss 0.0043\n",
            "Epoch 79 Batch 3 Loss 0.0043\n",
            "Epoch 79 Batch 4 Loss 0.0067\n",
            "Epoch 79 Estimated Loss 0.0040\n",
            "Time taken for this epoch 3.0166659355163574 sec\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.0014\n",
            "Epoch 80 Batch 1 Loss 0.0007\n",
            "Epoch 80 Batch 2 Loss 0.0124\n",
            "Epoch 80 Batch 3 Loss 0.0023\n",
            "Epoch 80 Batch 4 Loss 0.0039\n",
            "Epoch 80 Estimated Loss 0.0041\n",
            "Time taken for this epoch 3.053185224533081 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0021\n",
            "Epoch 81 Batch 1 Loss 0.0010\n",
            "Epoch 81 Batch 2 Loss 0.0137\n",
            "Epoch 81 Batch 3 Loss 0.0029\n",
            "Epoch 81 Batch 4 Loss 0.0087\n",
            "Epoch 81 Estimated Loss 0.0057\n",
            "Time taken for this epoch 2.9266107082366943 sec\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.0168\n",
            "Epoch 82 Batch 1 Loss 0.0006\n",
            "Epoch 82 Batch 2 Loss 0.0005\n",
            "Epoch 82 Batch 3 Loss 0.0047\n",
            "Epoch 82 Batch 4 Loss 0.0044\n",
            "Epoch 82 Estimated Loss 0.0054\n",
            "Time taken for this epoch 2.926060914993286 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0009\n",
            "Epoch 83 Batch 1 Loss 0.0097\n",
            "Epoch 83 Batch 2 Loss 0.0038\n",
            "Epoch 83 Batch 3 Loss 0.0105\n",
            "Epoch 83 Batch 4 Loss 0.0038\n",
            "Epoch 83 Estimated Loss 0.0057\n",
            "Time taken for this epoch 2.8699190616607666 sec\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.0028\n",
            "Epoch 84 Batch 1 Loss 0.0012\n",
            "Epoch 84 Batch 2 Loss 0.0011\n",
            "Epoch 84 Batch 3 Loss 0.0155\n",
            "Epoch 84 Batch 4 Loss 0.0008\n",
            "Epoch 84 Estimated Loss 0.0043\n",
            "Time taken for this epoch 2.8619911670684814 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0006\n",
            "Epoch 85 Batch 1 Loss 0.0104\n",
            "Epoch 85 Batch 2 Loss 0.0011\n",
            "Epoch 85 Batch 3 Loss 0.0051\n",
            "Epoch 85 Batch 4 Loss 0.0066\n",
            "Epoch 85 Estimated Loss 0.0048\n",
            "Time taken for this epoch 2.8951408863067627 sec\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.0012\n",
            "Epoch 86 Batch 1 Loss 0.0025\n",
            "Epoch 86 Batch 2 Loss 0.0005\n",
            "Epoch 86 Batch 3 Loss 0.0076\n",
            "Epoch 86 Batch 4 Loss 0.0075\n",
            "Epoch 86 Estimated Loss 0.0039\n",
            "Time taken for this epoch 2.8472936153411865 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0060\n",
            "Epoch 87 Batch 1 Loss 0.0038\n",
            "Epoch 87 Batch 2 Loss 0.0049\n",
            "Epoch 87 Batch 3 Loss 0.0007\n",
            "Epoch 87 Batch 4 Loss 0.0012\n",
            "Epoch 87 Estimated Loss 0.0033\n",
            "Time taken for this epoch 3.0122902393341064 sec\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.0010\n",
            "Epoch 88 Batch 1 Loss 0.0091\n",
            "Epoch 88 Batch 2 Loss 0.0046\n",
            "Epoch 88 Batch 3 Loss 0.0010\n",
            "Epoch 88 Batch 4 Loss 0.0005\n",
            "Epoch 88 Estimated Loss 0.0032\n",
            "Time taken for this epoch 3.091862916946411 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0006\n",
            "Epoch 89 Batch 1 Loss 0.0014\n",
            "Epoch 89 Batch 2 Loss 0.0102\n",
            "Epoch 89 Batch 3 Loss 0.0006\n",
            "Epoch 89 Batch 4 Loss 0.0030\n",
            "Epoch 89 Estimated Loss 0.0031\n",
            "Time taken for this epoch 3.039743661880493 sec\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.0007\n",
            "Epoch 90 Batch 1 Loss 0.0028\n",
            "Epoch 90 Batch 2 Loss 0.0008\n",
            "Epoch 90 Batch 3 Loss 0.0022\n",
            "Epoch 90 Batch 4 Loss 0.0041\n",
            "Epoch 90 Estimated Loss 0.0021\n",
            "Time taken for this epoch 2.9334380626678467 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0011\n",
            "Epoch 91 Batch 1 Loss 0.0005\n",
            "Epoch 91 Batch 2 Loss 0.0006\n",
            "Epoch 91 Batch 3 Loss 0.0035\n",
            "Epoch 91 Batch 4 Loss 0.0048\n",
            "Epoch 91 Estimated Loss 0.0021\n",
            "Time taken for this epoch 2.9620819091796875 sec\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.0005\n",
            "Epoch 92 Batch 1 Loss 0.0030\n",
            "Epoch 92 Batch 2 Loss 0.0008\n",
            "Epoch 92 Batch 3 Loss 0.0032\n",
            "Epoch 92 Batch 4 Loss 0.0015\n",
            "Epoch 92 Estimated Loss 0.0018\n",
            "Time taken for this epoch 2.8886611461639404 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0018\n",
            "Epoch 93 Batch 1 Loss 0.0008\n",
            "Epoch 93 Batch 2 Loss 0.0015\n",
            "Epoch 93 Batch 3 Loss 0.0027\n",
            "Epoch 93 Batch 4 Loss 0.0007\n",
            "Epoch 93 Estimated Loss 0.0015\n",
            "Time taken for this epoch 2.9301512241363525 sec\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.0004\n",
            "Epoch 94 Batch 1 Loss 0.0018\n",
            "Epoch 94 Batch 2 Loss 0.0006\n",
            "Epoch 94 Batch 3 Loss 0.0023\n",
            "Epoch 94 Batch 4 Loss 0.0013\n",
            "Epoch 94 Estimated Loss 0.0012\n",
            "Time taken for this epoch 2.9857089519500732 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0008\n",
            "Epoch 95 Batch 1 Loss 0.0005\n",
            "Epoch 95 Batch 2 Loss 0.0016\n",
            "Epoch 95 Batch 3 Loss 0.0009\n",
            "Epoch 95 Batch 4 Loss 0.0016\n",
            "Epoch 95 Estimated Loss 0.0011\n",
            "Time taken for this epoch 2.9593281745910645 sec\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.0004\n",
            "Epoch 96 Batch 1 Loss 0.0015\n",
            "Epoch 96 Batch 2 Loss 0.0007\n",
            "Epoch 96 Batch 3 Loss 0.0007\n",
            "Epoch 96 Batch 4 Loss 0.0014\n",
            "Epoch 96 Estimated Loss 0.0010\n",
            "Time taken for this epoch 2.8920462131500244 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0020\n",
            "Epoch 97 Batch 1 Loss 0.0006\n",
            "Epoch 97 Batch 2 Loss 0.0003\n",
            "Epoch 97 Batch 3 Loss 0.0009\n",
            "Epoch 97 Batch 4 Loss 0.0005\n",
            "Epoch 97 Estimated Loss 0.0009\n",
            "Time taken for this epoch 2.8297994136810303 sec\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.0014\n",
            "Epoch 98 Batch 1 Loss 0.0005\n",
            "Epoch 98 Batch 2 Loss 0.0003\n",
            "Epoch 98 Batch 3 Loss 0.0007\n",
            "Epoch 98 Batch 4 Loss 0.0010\n",
            "Epoch 98 Estimated Loss 0.0008\n",
            "Time taken for this epoch 2.7495932579040527 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0011\n",
            "Epoch 99 Batch 1 Loss 0.0011\n",
            "Epoch 99 Batch 2 Loss 0.0004\n",
            "Epoch 99 Batch 3 Loss 0.0005\n",
            "Epoch 99 Batch 4 Loss 0.0006\n",
            "Epoch 99 Estimated Loss 0.0007\n",
            "Time taken for this epoch 3.035283327102661 sec\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.0004\n",
            "Epoch 100 Batch 1 Loss 0.0007\n",
            "Epoch 100 Batch 2 Loss 0.0009\n",
            "Epoch 100 Batch 3 Loss 0.0009\n",
            "Epoch 100 Batch 4 Loss 0.0005\n",
            "Epoch 100 Estimated Loss 0.0007\n",
            "Time taken for this epoch 3.1791250705718994 sec\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.0004\n",
            "Epoch 101 Batch 1 Loss 0.0005\n",
            "Epoch 101 Batch 2 Loss 0.0006\n",
            "Epoch 101 Batch 3 Loss 0.0009\n",
            "Epoch 101 Batch 4 Loss 0.0009\n",
            "Epoch 101 Estimated Loss 0.0007\n",
            "Time taken for this epoch 3.0187113285064697 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iI09zf-bGsqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    fontdict = {'fontsize': 14}\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    plt.show()\n",
        "\n",
        "def ask(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    print('Question: %s' % (sentence))\n",
        "    print('Predicted answer: {}'.format(result))\n",
        "\n",
        "def show_attention_plot(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
        "\n",
        "def do_you_know(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkzhBscq3zWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "0ec3584f-e039-4869-be9d-9016a4076b8f"
      },
      "source": [
        "do_you_know(\"who plays young flo in the progressive commercials\")\n",
        "show_attention_plot(\"who plays young flo in the progressive commercials\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAFRCAYAAAASIlvpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqQGGOkmSpAYY6iRJkhpgqJMkSWrA/w82Ti7tQwX07AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "7a7fe15c-ca68-49f0-bec9-059d7ef47e6a",
        "_cell_guid": "4684e38d-ab84-492d-8e2f-d13b1a0924fd",
        "trusted": true,
        "id": "4hRJh7EFGsqw",
        "colab_type": "code",
        "outputId": "ddd43f18-4513-4b71-c70e-5c193e6eef1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "ask('which is the most common use of opt-in e-mail marketing')    \n",
        "ask('most common use of opt-in e-mail marketing')\n",
        "ask('how did I meet your mother')\n",
        "ask('who is your mother')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: <start> which is the most common use of opt in e mail marketing <end>\n",
            "Predicted answer: kevin skinner <end> \n",
            "Question: <start> most common use of opt in e mail marketing <end>\n",
            "Predicted answer: kevin skinner <end> \n",
            "Question: <start> how did i meet your mother <end>\n",
            "Predicted answer: october <end> \n",
            "Question: <start> who is your mother <end>\n",
            "Predicted answer: yokota air base , fussa , western tokyo <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f373ec7c-0e72-4802-a9a2-241027a3cea0",
        "_cell_guid": "c0b6a8d6-a947-4483-8897-a3cb7be8af6a",
        "trusted": true,
        "colab_type": "code",
        "id": "WyNnr-AIEgsG",
        "colab": {}
      },
      "source": [
        "print(\"going over all the questions and selecting those with answers ... \\n\")\n",
        "with open(file_to_read) as json_file:\n",
        "        data = json.load(json_file)\n",
        "        n_answers=0\n",
        "        for doc in data:\n",
        "            question_text = doc['question_text']\n",
        "            answer = do_you_know(question_text)\n",
        "            if answer != 'unknown <end> ': \n",
        "                n_answers+=1\n",
        "                ask(question_text)\n",
        "                print('\\n')\n",
        "        print(n_answers,\"answers out of\",len(data), \"possible, rate is\",100*n_answers/len(data),\"%\")\n",
        "           "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VbL6X2bzGsq3",
        "colab_type": "code",
        "outputId": "2e0ec1b9-e5ef-42f9-9ba8-30ebec98242d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "result, sentence, attention_plot = evaluate(\"who plays young flo in the progressive commercials\")\n",
        "show_attention_plot(sentence)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAEsCAYAAAClnkX2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iRhZwdJrWPG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e54d0ef-efb8-4d56-953e-88a5f7be8d01"
      },
      "source": [
        "print(\"going over all the questions and selecting those with answers ... \\n\")\n",
        "target, source, context = create_dataset()\n",
        "n_answers=0\n",
        "for i in range(len(source)):\n",
        "    question_text = source[i]\n",
        "    answer = do_you_know(question_text)\n",
        "    if answer != 'unknown <end> ': \n",
        "        n_answers+=1\n",
        "        ask(question_text)\n",
        "        print(\"The answer was:\",target[i])\n",
        "        print('\\n')\n",
        "\n",
        "print(n_answers,\"answers out of\",len(data), \"possible, rate is\",100*n_answers/len(data),\"%\\n\")\n",
        "           "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "going over all the questions and selecting those with answers ... \n",
            "\n",
            "70 short answers out of 101 possible long answers, rate is 69.3069306930693 %\n",
            "Question: <start> start when is the last episode of season of the walking dead end <end>\n",
            "Predicted answer: march , <end> \n",
            "The answer was: <start> march , <end>\n",
            "\n",
            "\n",
            "Question: <start> start in greek mythology who was the goddess of spring growth end <end>\n",
            "Predicted answer: october <end> \n",
            "The answer was: <start> persephone p r s f ni greek , also called kore k ri the maiden <end>\n",
            "\n",
            "\n",
            "Question: <start> start what is the name of the most important jewish text end <end>\n",
            "Predicted answer: immediately on announcement of six episodes to conclude the series <end> \n",
            "The answer was: <start> the shulchan aruch <end>\n",
            "\n",
            "\n",
            "Question: <start> start what is the name of spain s most famous soccer team end <end>\n",
            "Predicted answer: irish origin <end> \n",
            "The answer was: <start> real madrid <end>\n",
            "\n",
            "\n",
            "Question: <start> start when was the first robot used in surgery end <end>\n",
            "Predicted answer: immediately irish origin <end> \n",
            "The answer was: <start>  <end>\n",
            "\n",
            "\n",
            "Question: <start> start who sings the song i don t care i love it end <end>\n",
            "Predicted answer: icona pop and charli xcx <end> \n",
            "The answer was: <start> icona pop and charli xcx <end>\n",
            "\n",
            "\n",
            "Question: <start> start who are uncle owen and aunt beru related to end <end>\n",
            "Predicted answer: immediately on announcement of the election schedule by the commission <end> \n",
            "The answer was: <start> shmi <end>\n",
            "\n",
            "\n",
            "Question: <start> start where is zimbabwe located in the world map end <end>\n",
            "Predicted answer: immediately tobiko <end> \n",
            "The answer was: <start> in southern africa , between the zambezi and limpopo rivers , bordered by south africa , botswana , zambia and mozambique <end>\n",
            "\n",
            "\n",
            "Question: <start> start acres is equal to how many hectares end <end>\n",
            "Predicted answer: irish origin <end> \n",
            "The answer was: <start> square hectometre hm <end>\n",
            "\n",
            "\n",
            "Question: <start> start who has won a calendar grand slam in tennis end <end>\n",
            "Predicted answer: march , <end> \n",
            "The answer was: <start> unknown <end>\n",
            "\n",
            "\n",
            "Question: <start> start where was donovan mitchell picked in the draft end <end>\n",
            "Predicted answer: the roof of between mmhg <end> \n",
            "The answer was: <start> th <end>\n",
            "\n",
            "\n",
            "Question: <start> start where did the beatles final live performance take place end <end>\n",
            "Predicted answer: at savile row <end> \n",
            "The answer was: <start> the roof of the headquarters of the band s multimedia corporation apple corps at savile row <end>\n",
            "\n",
            "\n",
            "Question: <start> start who is the president of costa rica end <end>\n",
            "Predicted answer: october <end> \n",
            "The answer was: <start> luis guillermo solis rivera <end>\n",
            "\n",
            "\n",
            "Question: <start> start when did power rangers tv show come out end <end>\n",
            "Predicted answer: march , <end> \n",
            "The answer was: <start> august , <end>\n",
            "\n",
            "\n",
            "Question: <start> start how many beverly hills cops movies are there end <end>\n",
            "Predicted answer: irish origin <end> \n",
            "The answer was: <start> three <end>\n",
            "\n",
            "\n",
            "Question: <start> start who holds the most women s wimbledon titles end <end>\n",
            "Predicted answer: october <end> \n",
            "The answer was: <start> martina navratilova <end>\n",
            "\n",
            "\n",
            "Question: <start> start where does jinx you owe me a coke come from end <end>\n",
            "Predicted answer: immediately october <end> \n",
            "The answer was: <start> jinx is a children s game with varying rules and penalties that occur when two people unintentionally speak the same word or phrase simultaneously <end>\n",
            "\n",
            "\n",
            "Question: <start> start where does the last name hickey come from end <end>\n",
            "Predicted answer: irish origin <end> \n",
            "The answer was: <start> irish origin <end>\n",
            "\n",
            "\n",
            "Question: <start> start where is the greatest royal rumble taking place end <end>\n",
            "Predicted answer: immediately tobiko <end> \n",
            "The answer was: <start> jeddah , saudi arabia <end>\n",
            "\n",
            "\n",
            "Question: <start> start who sang my name is tallulah in bugsy malone end <end>\n",
            "Predicted answer: louise liberty williams <end> \n",
            "The answer was: <start> louise liberty williams <end>\n",
            "\n",
            "\n",
            "Question: <start> start what is andy s sisters name in toy story end <end>\n",
            "Predicted answer: immediately on announcement of the election schedule by announcement of the election schedule by announcement of the election schedule by announcement of the election schedule by announcement of the election \n",
            "The answer was: <start> molly <end>\n",
            "\n",
            "\n",
            "Question: <start> start who sings with shaggy on it wasn me end <end>\n",
            "Predicted answer: english jamaican singer rikrok <end> \n",
            "The answer was: <start> english jamaican singer rikrok <end>\n",
            "\n",
            "\n",
            "Question: <start> start what is the largest catholic high school in america end <end>\n",
            "Predicted answer: over <end> \n",
            "The answer was: <start> st . francis preparatory school <end>\n",
            "\n",
            "\n",
            "Question: <start> start who established the peoples republic of china in end <end>\n",
            "Predicted answer: over <end> \n",
            "The answer was: <start> mao zedong <end>\n",
            "\n",
            "\n",
            "Question: <start> start who sang take that look off your face end <end>\n",
            "Predicted answer: square hectometre hm <end> \n",
            "The answer was: <start> marti webb <end>\n",
            "\n",
            "\n",
            "Question: <start> start who plays percy in the lost city of z end <end>\n",
            "Predicted answer: yokota air base , fussa , western tokyo <end> \n",
            "The answer was: <start> charlie hunnam <end>\n",
            "\n",
            "\n",
            "Question: <start> start is stone cold steve austin in the longest yard end <end>\n",
            "Predicted answer: immediately tobiko <end> \n",
            "The answer was: <start> unknown <end>\n",
            "\n",
            "\n",
            "Question: <start> start who plays the first lady on house of cards end <end>\n",
            "Predicted answer: yokota air base , fussa , western tokyo <end> \n",
            "The answer was: <start> joanna c . going <end>\n",
            "\n",
            "\n",
            "Question: <start> start who owns st andrews golf course in scotland end <end>\n",
            "Predicted answer: immediately october <end> \n",
            "The answer was: <start> the st andrews links trust <end>\n",
            "\n",
            "\n",
            "Question: <start> start which city and state hosts the annual college world series end <end>\n",
            "Predicted answer: irish origin <end> \n",
            "The answer was: <start> omaha , nebraska <end>\n",
            "\n",
            "\n",
            "Question: <start> start who is ted talking about in how i met your mother end <end>\n",
            "Predicted answer: chris pine <end> \n",
            "The answer was: <start> tracy mcconnell <end>\n",
            "\n",
            "\n",
            "Question: <start> start when does grey s anatomy season premiere end <end>\n",
            "Predicted answer: september , <end> \n",
            "The answer was: <start> september , <end>\n",
            "\n",
            "\n",
            "Question: <start> start the partial pressure of carbon dioxide in arterial blood is approximately end <end>\n",
            "Predicted answer: at sea level mmhg in arterial blood is between mmhg and mmhg <end> \n",
            "The answer was: <start> at sea level mmhg in arterial blood is between mmhg and mmhg <end>\n",
            "\n",
            "\n",
            "Question: <start> start what was uncle jesse s original last name on full house end <end>\n",
            "Predicted answer: cochran <end> \n",
            "The answer was: <start> cochran <end>\n",
            "\n",
            "\n",
            "Question: <start> start who pays the judgements on the judge mathis show end <end>\n",
            "Predicted answer: immediately tobiko <end> \n",
            "The answer was: <start> greg mathis <end>\n",
            "\n",
            "\n",
            "Question: <start> start who made up the elf on the shelf end <end>\n",
            "Predicted answer: chanda bell <end> \n",
            "The answer was: <start> chanda bell <end>\n",
            "\n",
            "\n",
            "Question: <start> start does an american president have to be born in the united states end <end>\n",
            "Predicted answer: cochran <end> \n",
            "The answer was: <start> unknown <end>\n",
            "\n",
            "\n",
            "Question: <start> start who warned concord that the british were coming end <end>\n",
            "Predicted answer: yokota air base , fussa , western tokyo <end> \n",
            "The answer was: <start> samuel prescott <end>\n",
            "\n",
            "\n",
            "Question: <start> start who lasted the longest on alone season end <end>\n",
            "Predicted answer: david mcintyre <end> \n",
            "The answer was: <start> david mcintyre <end>\n",
            "\n",
            "\n",
            "Question: <start> start where is the us military base in japan end <end>\n",
            "Predicted answer: yokota air base , fussa , western tokyo <end> \n",
            "The answer was: <start> yokota air base , fussa , western tokyo <end>\n",
            "\n",
            "\n",
            "Question: <start> start who determines the size of the supreme court end <end>\n",
            "Predicted answer: irish origin <end> \n",
            "The answer was: <start> congress <end>\n",
            "\n",
            "\n",
            "Question: <start> start who is the actor that plays lucifer on tv end <end>\n",
            "Predicted answer: st . francis preparatory school <end> \n",
            "The answer was: <start> thomas john ellis <end>\n",
            "\n",
            "\n",
            "Question: <start> start who designed the national coat of arms of south africa end <end>\n",
            "Predicted answer: october <end> \n",
            "The answer was: <start> iaan bekker <end>\n",
            "\n",
            "\n",
            "Question: <start> start when does model code of conduct come into force end <end>\n",
            "Predicted answer: cochran <end> \n",
            "The answer was: <start> immediately on announcement of the election schedule by the commission <end>\n",
            "\n",
            "\n",
            "Question: <start> start when did now thats what i call music come out end <end>\n",
            "Predicted answer: november <end> \n",
            "The answer was: <start> november <end>\n",
            "\n",
            "\n",
            "Question: <start> start where is the grey cup being played end <end>\n",
            "Predicted answer: immediately tobiko <end> \n",
            "The answer was: <start> the brick field at commonwealth stadium in edmonton , alberta <end>\n",
            "\n",
            "\n",
            "Question: <start> start where did the band bastille get their name end <end>\n",
            "Predicted answer: irish origin <end> \n",
            "The answer was: <start> bastille day <end>\n",
            "\n",
            "\n",
            "Question: <start> start where is arkansas river located on a map end <end>\n",
            "Predicted answer: immediately october <end> \n",
            "The answer was: <start> the arkansas river flows through colorado , kansas , oklahoma , and arkansas , and its watershed also drains parts of texas , new mexico and missouri . <end>\n",
            "\n",
            "\n",
            "Question: <start> start who plays nicholas in the princess diaries end <end>\n",
            "Predicted answer: square hectometre hm <end> \n",
            "The answer was: <start> chris pine <end>\n",
            "\n",
            "\n",
            "Question: <start> start where is krishna river located on a map end <end>\n",
            "Predicted answer: march , <end> \n",
            "The answer was: <start> unknown <end>\n",
            "\n",
            "\n",
            "Question: <start> start the p wave phase of an electrocardiogram ecg represents end <end>\n",
            "Predicted answer: st . francis preparatory school <end> \n",
            "The answer was: <start> atrial depolarization <end>\n",
            "\n",
            "\n",
            "Question: <start> start when did star trek the next generation first air end <end>\n",
            "Predicted answer: november <end> \n",
            "The answer was: <start> september , <end>\n",
            "\n",
            "\n",
            "Question: <start> start why was the civil rights act of made into an amendment to the constitution end <end>\n",
            "Predicted answer: immediately on announcement of the election schedule by the election schedule by the election schedule by the election schedule by the election schedule by the election schedule by the election \n",
            "The answer was: <start> unknown <end>\n",
            "\n",
            "\n",
            "Question: <start> start who sang the most wonderful summer of my life end <end>\n",
            "Predicted answer: october <end> \n",
            "The answer was: <start> jackie ward <end>\n",
            "\n",
            "\n",
            "Question: <start> start how many oscars did on golden pond win end <end>\n",
            "Predicted answer: irish origin <end> \n",
            "The answer was: <start> three <end>\n",
            "\n",
            "\n",
            "Question: <start> start when did they figure out that yeast made bread rise end <end>\n",
            "Predicted answer: march , <end> \n",
            "The answer was: <start> unknown <end>\n",
            "\n",
            "\n",
            "Question: <start> start who won season of america s got talent end <end>\n",
            "Predicted answer: irish origin <end> \n",
            "The answer was: <start> kevin skinner <end>\n",
            "\n",
            "\n",
            "Question: <start> start how many episodes on queen of the south season end <end>\n",
            "Predicted answer: immediately october <end> \n",
            "The answer was: <start>  <end>\n",
            "\n",
            "\n",
            "Question: <start> start who is the existing prime minister of pakistan end <end>\n",
            "Predicted answer: october <end> \n",
            "The answer was: <start> imran khan <end>\n",
            "\n",
            "\n",
            "Question: <start> start who is known as the father of texas end <end>\n",
            "Predicted answer: october <end> \n",
            "The answer was: <start> stephen fuller austin <end>\n",
            "\n",
            "\n",
            "Question: <start> start what is the orange stuff on my sushi end <end>\n",
            "Predicted answer: irish origin <end> \n",
            "The answer was: <start> tobiko <end>\n",
            "\n",
            "\n",
            "Question: <start> start when did nsw last won a state of origin series end <end>\n",
            "Predicted answer: <end> \n",
            "The answer was: <start>  <end>\n",
            "\n",
            "\n",
            "Question: <start> start when does life is strange before the storm part end <end>\n",
            "Predicted answer: immediately irish origin <end> \n",
            "The answer was: <start> october <end>\n",
            "\n",
            "\n",
            "Question: <start> start where is sodom and gomorrah located in the bible end <end>\n",
            "Predicted answer: cochran <end> \n",
            "The answer was: <start> on the jordan river plain in the southern region of the land of canaan <end>\n",
            "\n",
            "\n",
            "Question: <start> start what is the longest non medical word in the dictionary end <end>\n",
            "Predicted answer: immediately on announcement of the election schedule by the commission <end> \n",
            "The answer was: <start> unknown <end>\n",
            "\n",
            "\n",
            "Question: <start> start when did the xbox slim come out end <end>\n",
            "Predicted answer: <end> \n",
            "The answer was: <start> in <end>\n",
            "\n",
            "\n",
            "Question: <start> start who spread the theory that one is a product of the mind and body end <end>\n",
            "Predicted answer: in <end> \n",
            "The answer was: <start> rene descartes <end>\n",
            "\n",
            "\n",
            "Question: <start> start how many goals scored ronaldo in his career end <end>\n",
            "Predicted answer: over <end> \n",
            "The answer was: <start> over <end>\n",
            "\n",
            "\n",
            "Question: <start> start who played tess on touched by an angel end <end>\n",
            "Predicted answer: singer chuck mosley <end> \n",
            "The answer was: <start> delloreese patricia early july , november , , known professionally as della reese <end>\n",
            "\n",
            "\n",
            "Question: <start> start when did the hornets move to new orleans end <end>\n",
            "Predicted answer: <end> \n",
            "The answer was: <start> season <end>\n",
            "\n",
            "\n",
            "Question: <start> start fish appeared in the fossil record during the end <end>\n",
            "Predicted answer: october <end> \n",
            "The answer was: <start> the cambrian explosion <end>\n",
            "\n",
            "\n",
            "Question: <start> start who died from the band faith no more end <end>\n",
            "Predicted answer: st . francis preparatory school <end> \n",
            "The answer was: <start> singer chuck mosley <end>\n",
            "\n",
            "\n",
            "Question: <start> start who signed the gun control act of end <end>\n",
            "Predicted answer: tobiko <end> \n",
            "The answer was: <start> president lyndon b . johnson <end>\n",
            "\n",
            "\n",
            "Question: <start> start where do frankenstein and the monster first meet end <end>\n",
            "Predicted answer: march , <end> \n",
            "The answer was: <start> the mountains <end>\n",
            "\n",
            "\n",
            "Question: <start> start the human tendency to mimic other people s behavior is an example of end <end>\n",
            "Predicted answer: real madrid <end> \n",
            "The answer was: <start> mirroring <end>\n",
            "\n",
            "\n",
            "Question: <start> start when does season five of the killing come out end <end>\n",
            "Predicted answer: irish origin <end> \n",
            "The answer was: <start> a fourth season consisting of six episodes to conclude the series <end>\n",
            "\n",
            "\n",
            "Question: <start> start how is the head of the church of england end <end>\n",
            "Predicted answer: immediately on announcement of the election schedule by the commission <end> \n",
            "The answer was: <start> the monarch is the supreme governor <end>\n",
            "\n",
            "\n",
            "77 answers out of 200 possible, rate is 38.5 %\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}